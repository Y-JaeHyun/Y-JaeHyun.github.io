---
layout:     post
author:     bcnote3314
title: NUMA(Non-Uniform Memory Access)
subtitle:  성능을 올리기 위한 삽질 기록
category: Note
tags: 		OS Experiences
---

# 개요

VTune이 제공 해준 성능 부하 2번째 컨텐츠는 NUMA 이다.  
NUMA 구조를 활용하기 위한 기본 개념이랑 관련 명령어등에 대해서 정리 한다.  


# NUMA

NUMA(Non Uniform Memeory Access)는 멀티 프로세서 시스템에서 사용되는 메모리 설계 방법이다.  
멀티 프로세서, 즉 CPU가 두개 이상인 시스템에서 메모리 접근을 어떻게 효율적으로 할수 있을까?  

이전 [Post](https://bcnote3314.github.io/experiences/2021/09/10/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%A0%95%EB%A6%AC/) 에서 이미 캐시에 대한 이야기를 다루며 메모리 접근 속도에 대해서 간접적으로 이야기 한바가 있다.  
메모리 접근 속도의 한계가 있기 때문에 CPU에 캐시를 두어 최대한 접근 횟수를 줄이기 위한 방법이었다.  

메모리 접근 속도에 의한 부하는 CPU가 2개 이상인 경우에 더욱 문제가 된다.  
CPU에서 메모리로 접근하는 통로가 한개일 때 1번 CPU가 메모리에 접근하여 사용중이라면 2번 메모리는 메모리 접근이 불가능하여 놀고 있게된다.  
이런 문제점을 각각의 CPU에 독립적인 메모리 영역을 구분하여 제공하는 하는 방법을 통해 해결하려 하는것이 NUMA이다.  

![NUMA](http://drive.google.com/uc?export=view&id=1HDeUYOGfAu903zegLkKza9YjMV6LOv0f)

각각의 CPU가 자신의 Local Memory를 두기 때문에 다른 CPU와 충돌로 인하여 대기하는 일이 없어질 수 있다.  
하지만 모든 해결책이 그러하듯(?) NUMA 역시도 한계가 있다.

예를들어 1번 CPU에 속하는 코어에 할당되어 동작하는 스레드에서 전역 변수에 접근하려고하는데 해당 전역 변수는 2번 CPU의 Local Memory에 존재한다고 하면 어떻게 접근을 해야할까?

직접 접근을 못하기 때문에 2번 CPU가 사용하지 않을때 Remote로 접근할 수 밖에 없으며 이는 성능에 좋지 않은 영향을 준다.  
최대한 Local Memory를 사용해도록 신경을 써야한다.  
(VTune에서는 메모리 접근 과정을 분석해서 Local 접근과 Remote접근 횟수에 대한 정보를 제공해주기 때문에 어떤 위치에서 Remote Memory 접근을 많이하는지 볼수 있다.)

# NUMA Policy

메모리를 할하는 위치를 결정해주는 policy가 4가지로 구분되어있다.  

1. Default : 프로세스가 실행되는 CPU의 로컬 노드를 우선 사용한다.  
2. Bind : 특정 노드를 지정해서 사용한다.
3. Preferred : 특정 노드를 우선해서 사용한다.
4. Interleaved : 동일한 비율로 Round Robin 형태로 사용한다.

앞서 이야기 한것과 같이 당연히 로컬 노드를 사용하는 형태를 만들어야 성능이 좋은 형태이다.  
어떤 정책을 쓰더라도 항상 로컬만 사용하는 프로그램을 만들기란 불가능하다고 본다.  
각각의 장단점을 파악하고 주어진 조건에 맞춰서 최대한 로컬 노드를 많이 만들수 있는 방법을 선택해야 할것이다.  

내 경험의 기준에서 가장 좋은 선택은 Bind 정책이었다.  
Bind는 특정 노드를 지정하기 때문에 어떤 CPU Core를 사용하게 될지 모르는 상황이라면 절대 사용해서는 안되는 정책일 것이다.  

하지만 나에게 주어진 환경은 모든 Thread가 각각 Config 파일에 지정된 번호를 매핑해서 사용하는 형태였기 때문에 각 스레드별로 Core 번호가 고정되어 있었다.  

즉, 각 스레드들의 Core번호를 기준으로 Node 번호를 지정하는 것으로 최대한 Local Node를 사용할수 있게 되었다.  

하지만 위험한 점은 Bind의 경우 항상 정해진 Node에 할당을 시도하기 때문에 특정 Thread들에서 메모리를 과도하게 사용하면 성능이 저하될것이며 Local Node의 메모리 크기를 초과하는 경우 데몬다운 등을 유발할 수 있기 때문에 반드시 관리가 되어야 한다.

NUMA 관련 api나 개념등은 아래 링크를 참고해서 진행했다.  

* [참고1 - linux numa manual](https://man7.org/linux/man-pages/man3/numa.3.html)
* [참고2 - numa 소개 post](https://jihooyim1.gitbooks.io/linuxbasic/content/contents/06.html)


# 기타 검토 사항

NUMA Policy 외에도 시험하면서 여러가지 검토 사항들이 있었다.  

- Data 영역 변수

malloc에 의한 Heap 변수는 NUMA 정책에 의거해 할당하는 것을 검증해서 확인했다.  
하지만 Stack, Data영역의 동작은 그렇지 않았다.  

Bind 형태로 명확한 Policy를 사용하더라도 기준이 모호하게 혼용되어 사용되는 형태로 보였다. (move_page 함수를 사용해 주소값을 Node로 변환하여 검증했었다.)  

우선 Data영역은 NUMA 정책에 영향을 받지 않으며 OS 별로 할당방식이 다르다.  
Linux의 경우 First Touch 방식을 따르며 처음 4KB(Page Size)까지의 전역변수는 main thread의 코어 번호에 따라 결정이된다.  
이때 한번도 해당 4KB내에 속하는 변수외의 아직 한번도 접근되지 않은 변수를 조회해보면 아직 그 어떤 노드에도 할당되지 않았음을 알수있다.  
이후에 처음 접근하는 access 코어에 따라 할당되는 위치가 결정된다.  

아래 그림은 실제 테스트 프로그램을 작성해 확인했던 부분이다.  

![memory](http://drive.google.com/uc?export=view&id=1IGM19VTOSR_N7CWZFGGtg_U0EEEHguj6)  
![node](http://drive.google.com/uc?export=view&id=18J7hemP2Yg8BUyS9NHoiO--oiNF-F9_3)  
![page](http://drive.google.com/uc?export=view&id=1qey8kfUOEeO6PjZYjtfQiBelrKD-r_yK)

첫번째 캡쳐는 실제 프로세스에서 사용중인 메모리 영역에 대한 정보이다.  
Data 영역으로 할당된 부분이 빨간색으로 표시된 부분이다.  

즉 시작 지점인 0x1849000 부터 +4K 지점인 0x1849fff 까지는 프로세스가 실행된 메모리를 따라 Node가 결정된다.  
두번쨰 캡쳐를 보면 4K 내의 주소값은 실행된 CPU가 속하는 Node를 표시하며 그 외의 영역은 -2로 결과가 확인된다. (-2는 메모리에 로드되어 있지 않는것을 의미한다.)  

이후 다른 CPU Core에서 해당 메모리에 접근하면 해당 CPU에 맞춰 Node가 결정된다.  

위의 정보들을 토대로 내린 결론은 Data영역은 항상 Local Node를 사용하는것을 보장하지 못한다.  
Cache와 관련된 이슈도 있고 NUMA Node도 보장하지 못하기 때문에 전역 변수의 사용은 최대한 지양하는 방향으로 개발이 필요하다. 


- Stack 영역 변수

Stack 영역의 경우 Function Call 시점에 할당 Node가 결정된다.  
Stack 영역에 대해서는 명확하게 어떤 조건을 따른다고 정리된 문서나 정보를 찾지 못했다.  
다양한 시험 케이스를 통해서 실제 할당 위치를 보고 추론 했기 때문에 정확한 정보는 아닐 수 있다. (단순 참고용)

우선 일정부분의 변수 까지는 부모 Thread의 numa 정책에 따라 할당이 되는것으로 확인했다.  
그 일정 부분이라는 값을 명확하게 찾지는 못했지만 Data와 마찬가지로  Page Size와 연관이 있을수 있을있을것으로 추정된다.

~~특정 위치 이후로 나오는 변수들은 해당 Thread의 numa 정책에 의해 할당된다.~~  
구조체 및 배열의 크기가 큰 변수 주변에 있는 변수들은 일부 Remote로 할당되는 현상이 확인되었다.  

Stack 영역의 Memory Node를 고정시킬 방법에 대해 찾아보면 prefault 라는 개념이 등장한다.  
prefault는 pagefault에 의해 의도하지 않게 성능이 저하되는 것을 막기 위해 사용하는 개념이다.  

최초 로드시 바로 pagefault를 시킨 이후 생성된 공간을 memory lock을 통해 페이징이 발생하지 않도록 하는 기법이다.  
이때 메모리 생성을 Bind 형태로 NUMA Node를 강제 할수 있다.  

[참고 자료1](https://stackoverflow.com/questions/13947446/stack-prefaulting-in-linux-single-or-multiple-faults-needed)  
[참고 자료2](https://stackoverflow.com/questions/5721655/what-is-the-best-way-to-prefault-in-the-stack-for-a-pthreads-thread)  
[참고 자료3](https://rt.wiki.kernel.org/index.php/Threaded_RT-application_with_memory_locking_and_stack_handling_example)  
[참고 자료4](https://stackoverflow.com/questions/10605766/allocating-a-threads-stack-on-a-specific-numa-memory)  

# 마무리

최종 적으로는 Policy에 대한 부분만 적용하였고 Prefault 와 같은 부분은 아직 반영하지 않았다.  
Prefault 등은  나중에 조금 더 명확하게 이해가 필요하며 검증도 좀더 면밀하게 할 필요가 있어보인다.  

Data와 Stack 영역은 Page Size와 뭔가 밀접한 연관 관계가 있어 보이는데 Page Size에 따라 성능이 어떻게 바뀔지도 파악이 필요하다.  

그 외에도 동작 과정에서 move_page 함수를 통해 Remote로 판단되는 메모리를 Local로 전환시켜주는 방법도 있기 때문에 다양한 방면으로 개선 포인트가 존재한다. (TODO...)
