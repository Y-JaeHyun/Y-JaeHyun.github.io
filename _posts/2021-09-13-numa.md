---
layout:     post
author:     bcnote3314
title:  성능을 올리기 위한 삽질 기록
subtitle: NUMA(Non-Uniform Memory Access)
category: Experiences
---

# 개요

VTune이 제공 해준 성능 부하 2번째 컨텐츠는 NUMA 이다.  
NUMA 구조를 활용하기 위한 기본 개념이랑 관련 명령어등에 대해서 정리 한다.  


# NUMA

NUMA(Non Uniform Memeory Access)는 멀티 프로세서 시스템에서 사용되는 메모리 설계 방법이다.  
멀티 프로세서, 즉 CPU가 두개 이상인 시스템에서 메모리 접근을 어떻게 효율적으로 할수 있을까?  

이전 [Post](https://bcnote3314.github.io/experiences/2021/09/10/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%A0%95%EB%A6%AC/) 에서 이미 캐시에 대한 이야기를 다루며 메모리 접근 속도에 대해서 간접적으로 이야기 한바가 있다.  
메모리 접근 속도의 한계가 있기 때문에 CPU에 캐시를 두어 최대한 접근 횟수를 줄이기 위한 방법이었다.  

메모리 접근 속도에 의한 부하는 CPU가 2개 이상인 경우에 더욱 문제가 된다.  
CPU에서 메모리로 접근하는 통로가 한개일 때 1번 CPU가 메모리에 접근하여 사용중이라면 2번 메모리는 메모리 접근이 불가능하여 놀고 있게된다.  
이런 문제점을 각각의 CPU에 독립적인 메모리 영역을 구분하여 제공하는 하는 방법을 통해 해결하려 하는것이 NUMA이다.  

!이미지 :!

각각의 CPU가 자신의 Local Memory를 두기 때문에 다른 CPU와 충돌로 인하여 대기하는 일이 없어질 수 있다.  
하지만 모든 해결책이 그러하듯(?) NUMA 역시도 한계가 있다.

예를들어 1번 CPU에 속하는 코어에 할당되어 동작하는 스레드에서 전역 변수에 접근하려고하는데 해당 전역 변수는 2번 CPU의 Local Memory에 존재한다고 하면 어떻게 접근을 해야할까?

직접 접근을 못하기 때문에 2번 CPU가 사용하지 않을때 Remote로 접근할 수 밖에 없으며 이는 성능에 좋지 않은 영향을 준다.  
최대한 Local Memory를 사용해도록 신경을 써야한다.  
(VTune에서는 메모리 접근 과정을 분석해서 Local 접근과 Remote접근 횟수에 대한 정보를 제공해주기 때문에 어떤 위치에서 Remote Memory 접근을 많이하는지 볼수 있다.)

# NUMA Policy

메모리를 할하는 위치를 결정해주는 policy가 4가지로 구분되어있다.  

1. Default : 프로세스가 실행되는 CPU의 로컬 노드를 우선 사용한다.  
2. Bind : 특정 노드를 지정해서 사용한다.
3. Preferred : 특정 노드를 우선해서 사용한다.
4. Interleaved : 동일한 비율로 Round Robin 형태로 사용한다.

앞서 이야기 한것과 같이 당연히 로컬 노드를 사용하는 형태를 만들어야 성능이 좋은 형태이다.  
어떤 정책을 쓰더라도 항상 로컬만 사용하는 프로그램을 만들기란 불가능하다고 본다.  
각각의 장단점을 파악하고 주어진 조건에 맞춰서 최대한 로컬 노드를 많이 만들수 있는 방법을 선택해야 할것이다.  

내 경험의 기준에서 가장 좋은 선택은 Bind 정책이었다.  
Bind는 특정 노드를 지정하기 때문에 어떤 CPU Core를 사용하게 될지 모르는 상황이라면 절대 사용해서는 안되는 정책일 것이다.  

하지만 나에게 주어진 환경은 모든 Thread가 각각 Config 파일에 지정된 번호를 매핑해서 사용하는 형태였기 때문에 각 스레드별로 Core 번호가 고정되어 있었다.  

즉, 각 스레드들의 Core번호를 기준으로 Node 번호를 지정하는 것으로 최대한 Local Node를 사용할수 있게 되었다.  

하지만 위험한 점은 Bind의 경우 항상 정해진 Node에 할당을 시도하기 때문에 특정 Thread들에서 메모리를 과도하게 사용하면 성능이 저하될것이며 Local Node의 메모리 크기를 초과하는 경우 데몬다운 등을 유발할 수 있기 때문에 반드시 관리가 되어야 한다.

NUMA 관련 api나 개념등은 아래 링크를 참고해서 진행했다.  

* [참고1 - linux numa manual](https://man7.org/linux/man-pages/man3/numa.3.html)
* [참고2 - numa 소개 post](https://jihooyim1.gitbooks.io/linuxbasic/content/contents/06.html)


# 기타 검토 사항

NUMA Policy 외에도 시험하면서 여러가지 검토 사항들이 있었다.  

1. Data 영역 변수

malloc에 의한 Heap 변수는 NUMA 정책에 의거해 할당하는 것을 검증해서 확인했다.  
하지만 Stack, Data영역의 동작은 그렇지 않았다.  

Bind 형태로 명확한 Policy를 사용하더라도 기준이 모호하게 혼용되어 사용되는 형태로 보였다. (move_page 함수를 사용해 주소값을 Node로 변환하여 검증했었다.)  

우선 Data영역은 NUMA 정책에 영향을 받지 않으며 OS 별로 할당방식이 다르다.  
Linux의 경우 First Touch 방식을 따르며 처음 4KB(Page Size)까지의 전역변수는 main thread의 코어 번호에 따라 결정이된다.  
이때 한번도 해당 4KB내에 속하는 변수외의 아직 한번도 접근되지 않은 변수를 조회해보면 아직 그 어떤 노드에도 할당되지 않았음을 알수있다.  
이후에 처음 접근하는 access 코어에 따라 할당되는 위치가 결정된다.  

아래 그림은 실제 테스트 프로그램을 작성해 확인했던 부분이다.  

[그림1] [그림2] [그림3]

대부분의 변수는 해당 스레드가 동작하는 코어의 로컬을 사용했으나 일부 몇몇 변수들의 경우에는 리모트를 사용한 현상을 보였다.  
결과로만 해석하면 최초 스레드 생성시점에 앞단에 존재하는 

2. Stack 영역 변수


# 메모리 


